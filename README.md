# Bert-Embedding-Recommender
natural-language-processing (NLP), recommendation-system, llm, bert, transformers


# ğŸ§  Token-Based Product Recommender using BERT

This project is an educational extension of **Chapter 2** from the *Hands-On Large Language Models* book. It demonstrates how to use token embeddings from a transformer model (BERT) to build a **text-based product recommendation system**.

---

## ğŸ“Œ Project Overview

We simulate a simple semantic search system where:
- A list of product names is embedded using a pre-trained transformer.
- A user query (e.g., â€œsporty footwearâ€) is converted to an embedding.
- Cosine similarity is used to recommend the most relevant products.
- The results are visualized using PCA to show how the model â€œthinksâ€ in vector space.

---

## ğŸ”§ Tech Stack

- [Transformers (ğŸ¤— Hugging Face)](https://huggingface.co/transformers/)
- PyTorch
- scikit-learn
- NumPy
- Matplotlib

---

## ğŸš€ How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/eddymontana/Bert-token-recommender.git
   cd Bert-token-recommender












## ğŸ“„ License

This project is licensed under the [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.  
Based on original materials from [Hands-On LLMs](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models).

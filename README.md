# Bert-Embedding-Recommender
natural-language-processing (NLP), recommendation-system, llm, bert, transformers


# 🧠 Token-Based Product Recommender using BERT

This project is an educational extension of **Chapter 2** from the *Hands-On Large Language Models* book. It demonstrates how to use token embeddings from a transformer model (BERT) to build a **text-based product recommendation system**.

---

## 📌 Project Overview

We simulate a simple semantic search system where:
- A list of product names is embedded using a pre-trained transformer.
- A user query (e.g., “sporty footwear”) is converted to an embedding.
- Cosine similarity is used to recommend the most relevant products.
- The results are visualized using PCA to show how the model “thinks” in vector space.

---

## 🔧 Tech Stack

- [Transformers (🤗 Hugging Face)](https://huggingface.co/transformers/)
- PyTorch
- scikit-learn
- NumPy
- Matplotlib

---

## 🚀 How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/eddymontana/Bert-token-recommender.git
   cd Bert-token-recommender












## 📄 License

This project is licensed under the [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.  
Based on original materials from [Hands-On LLMs](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models).
